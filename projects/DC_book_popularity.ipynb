{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "546f51db-fab9-4bc8-9a34-10ef0fa6d239",
      "metadata": {},
      "source": [
        "# Project - What makes a good book?\n",
        "\n",
        "Identifying popular products is incredibly important for e-commerce companies! Popular products generate more revenue and, therefore, play a key role in stock control.\n",
        "\n",
        "You've been asked to support an online bookstore by building a model to predict whether a book will be popular or not. They've supplied you with an extensive dataset containing information about all books they've sold, including:\n",
        "\n",
        "+ price\n",
        "+ popularity (target variable)\n",
        "+ review/summary\n",
        "+ review/text\n",
        "+ review/helpfulness\n",
        "+ authors\n",
        "+ categories\n",
        "+ \n",
        "You'll need to build a model that predicts whether a book will be rated as popular or not.\n",
        "\n",
        "They have high expectations of you, so have set a target of at least 70% accuracy! You are free to use as many features as you like, and will need to engineer new features to achieve this level of performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fe7d25-89ac-49d8-82e4-08742506f5d3",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "books = pd.read_csv(\"data/books.csv\")\n",
        "\n",
        "# Recode columns\n",
        "books.columns = ['title', 'price', 'review_helpfulness', 'review_summary', 'review_text',\n",
        "       'description', 'authors', 'categories', 'popularity']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e45889f-1364-416e-89dc-72fc3cbbb6bf",
      "metadata": {},
      "source": [
        "# Preprocessing\n",
        "\n",
        "+ Catagorical columns\n",
        "    + Rm non-alphanumeric characters and extra spaces from categorical text columns\n",
        "\n",
        "+ Numeric cols\n",
        "    + Convert the review_helpfulness column (e.g., \"3/5\") into a numerical fraction.\n",
        "    + Randomly impute missing values in review_helpfulness based on valid entries.\n",
        "    + Scale the price column using `StandardScaler`.\n",
        "    \n",
        "+ Combine text columns into a single review column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6524d3be-318b-44bf-98f7-219deebbd14a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Catagorical columns\n",
        "def clean_text(text):\n",
        "    cleaned = re.sub(r'[^a-zA-Z0-9]', '_', text)\n",
        "    cleaned = re.sub(r'_+', '_', cleaned)  # Remove consecutive underscores\n",
        "    return cleaned.strip('_')  # Remove leading and trailing underscores\n",
        "\n",
        "catagorical_df = books[['title', 'authors', 'categories']].applymap(clean_text)\n",
        "\n",
        "# Numeric columns\n",
        "def safe_fraction_conversion(x):\n",
        "    if '/' in x:\n",
        "        numerator, denominator = x.split('/')\n",
        "        numerator = float(numerator)\n",
        "        denominator = float(denominator)\n",
        "        return None if denominator == 0 else numerator / denominator\n",
        "    return float(x)\n",
        "\n",
        "books['review_helpfulness'] = books['review_helpfulness'].apply(safe_fraction_conversion)\n",
        "valid_values = books['review_helpfulness'].dropna()\n",
        "\n",
        "def random_imputer(val):\n",
        "    if np.isnan(val):\n",
        "        return np.random.choice(valid_values)  # Randomly choose from valid values\n",
        "    return val\n",
        "\n",
        "books['review_helpfulness'] = books['review_helpfulness'].apply(random_imputer)\n",
        "books['price'] = StandardScaler().fit_transform(books[['price']])\n",
        "\n",
        "# Text columns\n",
        "review_col_df = books[['review_summary', 'review_text', 'description']].fillna('')\n",
        "review_col_df['review'] = review_col_df.apply(\n",
        "    lambda x: ' '.join(x.dropna().astype(str)),\n",
        "    axis=1\n",
        ")\n",
        "review_col_df.drop(columns=['review_summary', 'review_text', 'description'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d373aa70-666f-472d-9b67-fa3cfb44c187",
      "metadata": {},
      "source": [
        "# Define Pipelines and run\n",
        "\n",
        "+ Pre-processing\n",
        "    + Scale numeric columns\n",
        "    + One-hot encode categorical columns \n",
        "    + Apply TF-IDF vectorization to the combined review text, extracting 2- and 3-gram features with a maximum of 100 features\n",
        "\n",
        "+ Classifiers\n",
        "    + Loop through a list of classifiers (RandomForestClassifier, LogisticRegression, and SVC).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81621cc5-a000-4774-89e6-728859bb75f4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "numeric_cols = ['price', 'review_helpfulness']\n",
        "categorical_cols = ['title', 'authors', 'categories']\n",
        "text_col = 'review'\n",
        "\n",
        "# Pre-processing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "        ('tfidf', TfidfVectorizer(max_features=100, stop_words='english', ngram_range=(2, 3)), text_col)\n",
        "    ]\n",
        ")\n",
        "\n",
        "#  Classifiers pipeline\n",
        "classifiers = [\n",
        "    ('Random Forest', RandomForestClassifier(max_depth=5, random_state=42)),\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('SVM', SVC(random_state=42))\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda32d65-b7de-4527-8240-83f905f4914e",
      "metadata": {},
      "source": [
        "# Train the model and evaluate the accuracy for each classifier\n",
        "\n",
        "+ Fit a Pipeline for each classifier, which applies the preprocessing steps and then fits the classifier on the data.\n",
        "+ Split the data into training and test sets and evaluate the accuracy for each classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29f7267-47c7-49ac-ad60-29b298f95160",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for name, clf in classifiers:\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "    # Split dataset into training and testing sets\n",
        "    X = books.drop(columns='popularity')\n",
        "    y = pd.get_dummies(books['popularity'], drop_first=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Train and evaluate each classifier\n",
        "    print(f\"Testing {name}...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy for {name}: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "raw",
      "id": "b4ed82c9-fb5e-4ba9-9996-e576c0ec7a60",
      "metadata": {},
      "source": [
        "Testing Random Forest...\n",
        "Accuracy for Random Forest: 0.7106\n",
        "Testing Logistic Regression...\n",
        "Accuracy for Logistic Regression: 0.7142\n",
        "Testing SVM...\n",
        "Accuracy for SVM: 0.6959"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede976fb-234d-4607-8bad-76fe1923a57a",
      "metadata": {
        "trusted": true
      },
      "source": [
        "# Evaluate feature importance for RandomForest (extra)\n",
        "\n",
        "+ Fit a RandomForestClassifier using the pipeline.\n",
        "+ Extract the feature importances after training.\n",
        "+ Display the top 10 most important features from the model.\n",
        "+ Why: Feature importance helps identify which aspects of the data (e.g., price, text n-grams, categories) have the most predictive power for the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dece4c0e-8595-4998-bb86-b656ca653a8b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "rf_clf = RandomForestClassifier(max_depth=5, random_state=42)\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', rf_clf)\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Extract feature importances from Random Forest\n",
        "rf_clf = pipeline.named_steps['classifier']\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "# Sort and display top 10 features\n",
        "feature_names = np.concatenate([numeric_cols, preprocessor.named_transformers_['cat'].get_feature_names_out(), preprocessor.named_transformers_['tfidf'].get_feature_names_out()])\n",
        "top_10_features = pd.Series(importances, index=feature_names).nlargest(10)\n",
        "print(\"Top 10 important features:\")\n",
        "print(top_10_features)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "29744c9e-5d83-4250-a1db-857c5302ee3a",
      "metadata": {},
      "source": [
        "Top 10 important features:\n",
        "review_helpfulness    0.067064\n",
        "price                 0.033818\n",
        "highly recommend      0.011156\n",
        "great book            0.010715\n",
        "read book             0.008981\n",
        "love book             0.006391\n",
        "book read             0.005926\n",
        "good book             0.005733\n",
        "reading book          0.005596\n",
        "excellent book        0.005308"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f62ebd",
      "metadata": {},
      "source": [
        "# Final Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "650c2eed-ecf4-40d3-b33d-2c35f59652aa",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Final Submission\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "books = pd.read_csv(\"data/books.csv\")\n",
        "books.columns = ['title', 'price', 'review_helpfulness', 'review_summary', 'review_text',\n",
        "       'description', 'authors', 'categories', 'popularity']\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9]', '_', re.sub(r'_+', '_', text)).strip('_')\n",
        "\n",
        "# Apply text cleaning on categorical columns\n",
        "books['title'] = books['title'].apply(clean_text)\n",
        "books['authors'] = books['authors'].apply(clean_text)\n",
        "books['categories'] = books['categories'].apply(clean_text)\n",
        "\n",
        "# Convert review_helpfulness to numeric, coercing errors to NaN\n",
        "books['review_helpfulness'] = pd.to_numeric(books['review_helpfulness'].apply(safe_fraction_conversion), errors='coerce')\n",
        "\n",
        "# Impute missing values in review_helpfulness\n",
        "valid_values = books['review_helpfulness'].dropna()\n",
        "books['review_helpfulness'] = books['review_helpfulness'].apply(lambda val: np.random.choice(valid_values) if pd.isna(val) else val)\n",
        "\n",
        "# Scale the price column\n",
        "books['price'] = StandardScaler().fit_transform(books[['price']])\n",
        "\n",
        "# Combine text columns and process text data\n",
        "review_col_df = books[['review_summary', 'review_text', 'description']].fillna('')\n",
        "review_col_df['review'] = review_col_df.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
        "books['review'] = review_col_df['review']\n",
        "\n",
        "# Define numeric, categorical, and text columns\n",
        "numeric_cols = ['price', 'review_helpfulness']\n",
        "categorical_cols = ['title', 'authors', 'categories']\n",
        "text_col = 'review'\n",
        "\n",
        "# Set up preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "        ('tfidf', TfidfVectorizer(max_features=100, stop_words='english', ngram_range=(2, 3)), text_col)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Build the final pipeline with Logistic Regression\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "# Prepare input X and target y\n",
        "X = books.drop(columns='popularity')\n",
        "y = pd.get_dummies(books['popularity'], drop_first=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fit the pipeline and assess accuracy\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "model_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Accuracy for Logistic Regression: {accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1be28c",
      "metadata": {},
      "source": [
        "# Suggested Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d19dcc-89c0-4c19-83e6-9c6205549ff0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import some required packages\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "# Read in the dataset\n",
        "books = pd.read_csv(\"data/books.csv\")\n",
        "\n",
        "# Inspect the DataFrame\n",
        "books.info()\n",
        "\n",
        "# Visualize popularity frequencies\n",
        "sns.countplot(data=books, x=\"popularity\")\n",
        "plt.show()\n",
        "\n",
        "# Check frequencies\n",
        "print(books[\"categories\"].value_counts())\n",
        "\n",
        "# Filter out rare categories to avoid overfitting\n",
        "books = books.groupby(\"categories\").filter(lambda x: len(x) > 100)\n",
        "\n",
        "# One-hot encoding categories\n",
        "categories = pd.get_dummies(books[\"categories\"], drop_first=True)\n",
        "\n",
        "# Bring categories into the DataFrame\n",
        "books = pd.concat([books, categories], axis=1)\n",
        "\n",
        "# Remove original column\n",
        "books.drop(columns=[\"categories\"], inplace=True)\n",
        "\n",
        "# Get number of total reviews \n",
        "books[\"num_reviews\"] = books[\"review/helpfulness\"].str.split(\"/\", expand=True)[1]\n",
        "\n",
        "# Get number of helpful reviews \n",
        "books[\"num_helpful\"] = books[\"review/helpfulness\"].str.split(\"/\", expand=True)[0]\n",
        "\n",
        "# Convert to integer datatype\n",
        "for col in [\"num_reviews\", \"num_helpful\"]:\n",
        "    books[col] = books[col].astype(int)\n",
        "    \n",
        "# Add percentage of helpful reviews as a column to normalize the data\n",
        "books[\"perc_helpful_reviews\"] = books[\"num_helpful\"] / books[\"num_reviews\"]\n",
        "\n",
        "# Fill null values\n",
        "books[\"perc_helpful_reviews\"].fillna(0, inplace=True)\n",
        "\n",
        "# Drop original column\n",
        "books.drop(columns=[\"review/helpfulness\"], inplace=True)\n",
        "\n",
        "# Convert strings to lowercase\n",
        "for col in [\"review/summary\", \"review/text\", \"description\"]:\n",
        "    books[col] = books[col].str.lower()\n",
        "    \n",
        "# Create a list of positive words to measure positive text sentiment\n",
        "positive_words = [\"great\", \"excellent\", \"good\", \"interesting\", \"enjoy\", \"helpful\", \"useful\", \"like\", \"love\", \"beautiful\", \"fantastic\", \"perfect\", \"wonderful\", \"impressive\", \"amazing\", \"outstanding\", \"remarkable\", \"brilliant\", \"exceptional\", \"positive\",\n",
        "    \"thrilling\"]\n",
        "\n",
        "# Instantiate a CountVectorizer\n",
        "vectorizer = CountVectorizer(vocabulary=positive_words)\n",
        "\n",
        "# Fit and transform review/text \n",
        "review_text = books[\"review/text\"]\n",
        "text_transformed = vectorizer.fit_transform(review_text.fillna(''))\n",
        "\n",
        "# Fit and transform review/summary\n",
        "review_summary = books[\"review/summary\"]\n",
        "summary_transformed = vectorizer.fit_transform(review_summary.fillna(''))\n",
        "\n",
        "# Fit and transform description\n",
        "description = books[\"description\"]\n",
        "description_transformed = vectorizer.fit_transform(description.fillna(''))\n",
        "\n",
        "# Add positive counts into DataFrame to add measures of positive sentiment\n",
        "books[\"positive_words_text\"] = text_transformed.sum(axis=1).reshape(-1, 1)\n",
        "books[\"positive_words_summary\"] = summary_transformed.sum(axis=1).reshape(-1, 1)\n",
        "books[\"positive_words_description\"] = description_transformed.sum(axis=1).reshape(-1, 1)\n",
        "\n",
        "# Remove original columns\n",
        "books.drop(columns=[\"review/text\", \"review/summary\", \"description\"], inplace=True)\n",
        "\n",
        "# Splitting into features and target values\n",
        "X = books.drop(columns=[\"title\", \"authors\", \"popularity\"]).values\n",
        "y = books[\"popularity\"].values.reshape(-1, 1)\n",
        "\n",
        "# Splitting into training and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Instantiate and fit a Random Forest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=120, max_depth=50, min_samples_split=5, random_state=42, class_weight=\"balanced\")\n",
        "clf.fit(X_train, y_train.ravel()) \n",
        "\n",
        "# Evaluate accuracy\n",
        "print(clf.score(X_train, y_train))\n",
        "print(clf.score(X_test, y_test))\n",
        "\n",
        "model_accuracy = clf.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "9316dc1b-af15-446c-b253-3093b1e132a2",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 15719 entries, 0 to 15718\n",
        "Data columns (total 9 columns):\n",
        " #   Column              Non-Null Count  Dtype  \n",
        "---  ------              --------------  -----  \n",
        " 0   title               15719 non-null  object \n",
        " 1   price               15719 non-null  float64\n",
        " 2   review/helpfulness  15719 non-null  object \n",
        " 3   review/summary      15719 non-null  object \n",
        " 4   review/text         15719 non-null  object \n",
        " 5   description         15719 non-null  object \n",
        " 6   authors             15719 non-null  object \n",
        " 7   categories          15719 non-null  object \n",
        " 8   popularity          15719 non-null  object \n",
        "dtypes: float64(1), object(8)\n",
        "memory usage: 1.1+ MB\n",
        "\n",
        "'Fiction'                      3520\n",
        "'Religion'                     1053\n",
        "'Biography & Autobiography'     852\n",
        "'Juvenile Fiction'              815\n",
        "'History'                       754\n",
        "                               ... \n",
        "'Sunflowers'                      1\n",
        "'Self-confidence'                 1\n",
        "'United States'                   1\n",
        "'Note-taking'                     1\n",
        "'Asthma'                          1\n",
        "Name: categories, Length: 313, dtype: int64\n",
        "0.9617126389460683\n",
        "0.7090036014405763"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "191fbb5f",
      "metadata": {},
      "source": [
        "# Differences between my solution and suggestion solution\n",
        "\n",
        "The main differences between you approach and the suggested solution lie in the preprocessing steps, feature engineering, and the choice of the classification model. Here’s a breakdown of the key differences that could explain why your Logistic Regression pipeline resulted in slightly higher accuracy (0.7142 vs. 0.7090) compared to the Random Forest model in the suggested solution:\n",
        "\n",
        "#### 1. Review Helpfulness Handling:\n",
        "\n",
        "Your Approach:\n",
        "\n",
        "You used a function (safe_fraction_conversion) to handle the review_helpfulness column, converted it to numeric, and applied imputation to handle missing values by randomly selecting valid values. This approach keeps the column as a continuous numeric feature.\n",
        "\n",
        "Suggested Approach:\n",
        "\n",
        "The suggested solution splits review_helpfulness into two parts: num_reviews and num_helpful, converting them into integers. Then, it calculates the percentage of helpful reviews (perc_helpful_reviews), which adds a normalized feature to the dataset.\n",
        "Impact: Both approaches handle review_helpfulness, but your approach keeps it in its original form (after coercion to numeric), while the suggested approach normalizes the data and drops the original column.\n",
        "\n",
        "#### 2. Text Preprocessing:\n",
        "\n",
        "Your Approach:\n",
        "\n",
        "You used a TfidfVectorizer with n-grams (bigrams and trigrams) to transform the text data into numerical features. This approach captures term frequency-inverse document frequency and n-gram context in the text data.\n",
        "\n",
        "Suggested Approach:\n",
        "\n",
        "The suggested solution uses a CountVectorizer with a predefined list of positive words, transforming the text into sentiment-related features. The counts of positive words are added as new features for review_text, review_summary, and description, after which the original text columns are dropped.\n",
        "Impact: Your approach uses more comprehensive text feature extraction (TF-IDF with n-grams), which may capture a wider range of text-based information, while the suggested approach focuses only on positive sentiment-related words, which limits the scope of text features.\n",
        "\n",
        "3. Categorical Data Handling:\n",
        "Your Approach:\n",
        "You applied OneHotEncoder to categorical columns such as title, authors, and categories as part of your preprocessing pipeline. This ensures that all unique categories are represented as binary vectors.\n",
        "Suggested Approach:\n",
        "The suggested solution filters out rare categories in the categories column (keeping only those with more than 100 samples) and then applies one-hot encoding. However, the title and authors columns are not encoded and are dropped in the final dataset.\n",
        "Impact: Your approach retains more information by encoding all categorical variables, while the suggested solution drops some useful categorical columns (such as title and authors), which may reduce the model's ability to capture useful patterns from those fields.\n",
        "\n",
        "### 4. Numerical Feature Scaling:\n",
        "\n",
        "Your Approach:\n",
        "\n",
        "You scaled numerical features like price using StandardScaler to normalize them.\n",
        "\n",
        "Suggested Approach:\n",
        "\n",
        "There is no explicit mention of feature scaling in the suggested approach, meaning numerical values (like price) are likely used as-is without scaling.\n",
        "Impact: Normalizing numerical values in your approach may contribute to a more consistent input range, which could lead to better performance for classifiers that are sensitive to feature scales (e.g., Logistic Regression). In contrast, the suggested solution doesn’t appear to normalize numeric data.\n",
        "\n",
        "5. Model Choice:\n",
        "\n",
        "Your Approach:\n",
        "\n",
        "You used Logistic Regression as the classifier. Logistic Regression is a linear model, and it often works well with data that has been thoroughly preprocessed, especially with normalized numerical data and TF-IDF features.\n",
        "\n",
        "Suggested Approach:\n",
        "\n",
        "The suggested solution uses a Random Forest Classifier with hyperparameters such as n_estimators=120, max_depth=50, and min_samples_split=5. Random Forests are robust and tend to perform well when handling complex datasets with both categorical and numerical features, but they might struggle when overfitting on rare categories.\n",
        "\n",
        "Impact: Logistic Regression might have worked better due to your comprehensive text processing and feature scaling. The Random Forest, while powerful, could have been affected by overfitting, especially given the heavy use of positive sentiment-based word counts in the suggested solution.\n",
        "\n",
        "#### 6. Imputation:\n",
        "\n",
        "Your Approach:\n",
        "\n",
        "You used imputation to fill missing values in review_helpfulness, ensuring no missing values remained before the modeling stage.\n",
        "\n",
        "Suggested Approach:\n",
        "\n",
        "The suggested solution doesn't explicitly handle missing data, which may leave some issues in the dataset that Random Forest can handle more gracefully than Logistic Regression, but this may still lead to inaccuracies.\n",
        "Impact: Your approach explicitly handles missing values, ensuring clean data for Logistic Regression, which is more sensitive to missing data than Random Forests.\n",
        "\n",
        "#### 7. Feature Selection:\n",
        "\n",
        "Your Approach:\n",
        "\n",
        "You allowed the classifier to consider all available features, including scaled numeric values, one-hot encoded categorical data, and TF-IDF transformed text features.\n",
        "\n",
        "Suggested Approach:\n",
        "\n",
        "The suggested solution specifically creates sentiment-related features based on predefined positive words, which limits the variety of features the model can use.\n",
        "Impact: Your approach generates a richer feature set, allowing the classifier to capture more detailed patterns in the data, while the suggested approach limits the features to sentiment-based counts, potentially missing out on other important signals.\n",
        "\n",
        "Why Your Accuracy Is Higher:\n",
        "\n",
        "Text Feature Richness: Your use of TF-IDF and n-grams likely captured more nuanced information from the text fields compared to the sentiment-focused CountVectorizer in the suggested approach.\n",
        "\n",
        "Feature Scaling: Logistic Regression tends to perform better with properly scaled data, and your scaling of numeric values like price may have contributed to better accuracy.\n",
        "\n",
        "Handling of Missing Values: The clear imputation strategy for missing review_helpfulness values may have contributed to better model performance by reducing the noise from missing or invalid data.\n",
        "\n",
        "While the suggested approach may work well for a Random Forest model, your approach's richer preprocessing, especially for text, and the use of a linear model like Logistic Regression with feature scaling seem to be the reasons behind the slightly higher accuracy."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
